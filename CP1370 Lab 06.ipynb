{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc09118b-8caf-4dce-84b6-00b77ef824a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Load the flight-summary.csv into a dataframe: ( be sure to use the correct path )\n",
    "\n",
    "This data represents the number of flights taking place from origin airports to destination\n",
    "airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa3e419-0162-42e5-a3c0-63f105ff4419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+-----+\n|origin_code|      origin_airport| origin_city|origin_state|dest_code|        dest_airport|dest_city|dest_state|count|\n+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+-----+\n|        BQN|Rafael Hernández ...|   Aguadilla|          PR|      MCO|Orlando Internati...|  Orlando|        FL|  441|\n|        PHL|Philadelphia Inte...|Philadelphia|          PA|      MCO|Orlando Internati...|  Orlando|        FL| 4869|\n|        MCI|Kansas City Inter...| Kansas City|          MO|      IAH|George Bush Inter...|  Houston|        TX| 1698|\n|        SPI|Abraham Lincoln C...| Springfield|          IL|      ORD|Chicago O'Hare In...|  Chicago|        IL|  998|\n|        SNA|John Wayne Airpor...|   Santa Ana|          CA|      PHX|Phoenix Sky Harbo...|  Phoenix|        AZ| 3846|\n+-----------+--------------------+------------+------------+---------+--------------------+---------+----------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_summary = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/flight_summary.csv\")\n",
    "\n",
    "flight_summary.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0405c51c-d1fd-43bd-b5f0-388c93db82e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write a query which determines how many unique origin airports are contained in the\n",
    "data. ( 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c6d799-50a5-482c-8b91-c2e01b126659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique origin airports:  322\n"
     ]
    }
   ],
   "source": [
    "unique_origins = flight_summary.select(\"origin_code\").distinct().count()\n",
    "print(\"Unique origin airports: \", unique_origins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015a71f7-2859-4605-8712-979bc39f2c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Modify query #2 to use the approx._count_distinct function with a margin of error of\n",
    "10%. (3 marks) Why does this function exists as it is not completely accurate? ( 2\n",
    "marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8751f9c0-753e-46e1-9116-84037b926ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|approx_count|\n+------------+\n|         318|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "approx_unique = flight_summary.agg(approx_count_distinct(\"origin_code\").alias(\"approx_count\")).show()\n",
    "\n",
    "# Approx count function exists because on large datasets calculating an exact value can be expensive. Using approximation values on these large datasets can still # give you a fairly accurate result as well for most use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4070107-a743-4ab3-8151-f6505fff810c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. What does the function skewness determine? (2 marks) Write a query which outputs\n",
    "the skewness of the “count” column. (3 marks) What does the result indicate? ( 2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79e9f34-5a9b-4387-94d1-9c0a61ab6458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|   skewness_value|\n+-----------------+\n|2.682183800064101|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Skewness measures the asymmetry of unbalance of the distribution of a variable. A value of zero indicates a balanced distribution. A positive value means the data is skewed to the right, a negative value to the left\n",
    "\n",
    "skewness = flight_summary.select(skewness(\"count\").alias(\"skewness_value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333cd3d9-555e-4f52-b2b9-892b2c120f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Write a query which outputs the top 5 most popular destination cities. You will need to\n",
    "group the data by destination state and destination city. ( 5 marks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bc8356-fe37-42ea-a260-b95fef11953b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------------+\n|dest_state|        dest_city|total_flights|\n+----------+-----------------+-------------+\n|        IL|          Chicago|       366790|\n|        GA|          Atlanta|       346904|\n|        TX|Dallas-Fort Worth|       239582|\n|        TX|          Houston|       198724|\n|        CO|           Denver|       196010|\n+----------+-----------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_destinations = flight_summary.groupBy(\"dest_state\", \"dest_city\") \\\n",
    "    .agg(sum(\"count\").alias(\"total_flights\")) \\\n",
    "    .orderBy(\"total_flights\", ascending=False) \\\n",
    "    .limit(5)\n",
    "\n",
    "top_destinations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32de17be-62e0-4d3d-a76a-0531e3e4de6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Write a query which groups the data by each origin airport and outputs the sum,\n",
    "average and standard deviation of the count column. Use the “.agg” function ( 5 marks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ab5610-ba38-4a03-895b-ef259c46ee58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------------+------------------+\n|origin_code|sum_count|         avg_count|      stddev_count|\n+-----------+---------+------------------+------------------+\n|        BGM|      262|             262.0|              null|\n|        DLG|       77|              77.0|              null|\n|        PSE|      749|             374.5| 58.68986283848344|\n|        INL|      574|191.33333333333334| 169.5120448031152|\n|        MSY|    38804|1021.1578947368421|1080.0178308416896|\n|        PPG|      107|             107.0|              null|\n|        GEG|     9505|             950.5| 878.9061699382679|\n|        SNA|    37187|1690.3181818181818| 1288.817966619899|\n|        BUR|    18889|1717.1818181818182|1015.1333723390063|\n|        GRB|     4881|           1220.25|1066.3112037924639|\n|        GTF|     1966| 655.3333333333334|308.61356634686905|\n|        IDA|     2247|             749.0| 665.8498329203064|\n|        GRR|    10845|             723.0| 714.2099531250296|\n|        JLN|      666|             666.0|              null|\n|        EUG|     3632|             726.4| 686.1470687833622|\n|        PSG|      664|             332.0|               0.0|\n|        PVD|    11058|             737.2|  721.932248502982|\n|        MYR|     4831| 284.1764705882353| 338.4357832909586|\n|        GSO|     6737| 561.4166666666666| 768.8377838522167|\n|        OAK|    42316|1209.0285714285715|1429.5528606919731|\n+-----------+---------+------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "origin_aggs = flight_summary.groupBy(\"origin_code\") \\\n",
    "    .agg(\n",
    "        sum(\"count\").alias(\"sum_count\"),\n",
    "        avg(\"count\").alias(\"avg_count\"),\n",
    "        stddev(\"count\").alias(\"stddev_count\")\n",
    "    )\n",
    "\n",
    "origin_aggs.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CP1370 Lab 06",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
