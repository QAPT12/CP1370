{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa70dd6-68b6-4bbf-8aeb-aa5aef26047c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, round, avg, col, weekofyear, min\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae8fcc68-bc77-4875-83bb-d59ca29be594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(5 marks)The file mpg.csv contains data about various automobiles\n",
    "Load this file into a dataframe. Group by the manufacturer and use a\n",
    "collection method to add a column which shows all unique models that\n",
    "manufacturer has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4626646a-96e0-4013-89ca-96ed3c9ae3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------------------------------------------------+\n|manufacturer|unique_models                                                                         |\n+------------+--------------------------------------------------------------------------------------+\n|land rover  |[range rover]                                                                         |\n|pontiac     |[grand prix]                                                                          |\n|toyota      |[4runner 4wd, camry solara, corolla, land cruiser wagon 4wd, toyota tacoma 4wd, camry]|\n|lincoln     |[navigator 2wd]                                                                       |\n|audi        |[a4, a6 quattro, a4 quattro]                                                          |\n|jeep        |[grand cherokee 4wd]                                                                  |\n|dodge       |[durango 4wd, ram 1500 pickup 4wd, dakota pickup 4wd, caravan 2wd]                    |\n|hyundai     |[sonata, tiburon]                                                                     |\n|ford        |[explorer 4wd, f150 pickup 4wd, expedition 2wd, mustang]                              |\n|chevrolet   |[k1500 tahoe 4wd, c1500 suburban 2wd, corvette, malibu]                               |\n|honda       |[civic]                                                                               |\n|volkswagen  |[jetta, passat, new beetle, gti]                                                      |\n|mercury     |[mountaineer 4wd]                                                                     |\n|nissan      |[altima, maxima, pathfinder 4wd]                                                      |\n|subaru      |[impreza awd, forester awd]                                                           |\n+------------+--------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "mpg = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/mpg.csv\")\n",
    "\n",
    "unique_models = mpg.groupBy(\"manufacturer\").agg(collect_set(\"model\").alias(\"unique_models\"))\n",
    "unique_models.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09bea397-a9e0-41e6-8f59-e34d092b2683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(5 marks) For the dataframe loaded from the mpg.csv file, generate a\n",
    "dataframe which is grouped by the year and cylinder (cyl) columns. Use the\n",
    "pivot method to add columns for the average city mpg(cty) for each\n",
    "manufacturer. Round the average city mpg to 1 decimal place to make the\n",
    "output easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a288a21b-67a1-45ff-9e88-ab7a9dfdb6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---------+-----+----+-----+-------+----+----------+-------+-------+------+-------+------+------+----------+\n|year|cyl|audi|chevrolet|dodge|ford|honda|hyundai|jeep|land rover|lincoln|mercury|nissan|pontiac|subaru|toyota|volkswagen|\n+----+---+----+---------+-----+----+-----+-------+----+----------+-------+-------+------+-------+------+------+----------+\n|2008|8  |16.0|13.6     |11.9 |13.6|null |null   |11.8|12.0      |12.0   |13.0   |12.0  |16.0   |null  |13.5  |null      |\n|1999|4  |18.3|19.0     |18.0 |null|24.8 |18.5   |null|null      |null   |null   |20.0  |null   |19.0  |20.0  |23.3      |\n|1999|6  |16.2|18.0     |14.9 |15.3|null |18.0   |15.0|null      |null   |14.0   |16.5  |17.0   |null  |16.5  |16.8      |\n|2008|6  |16.8|17.5     |15.1 |15.3|null |17.3   |16.0|null      |null   |13.0   |17.8  |18.0   |null  |16.8  |17.0      |\n|2008|5  |null|null     |null |null|null |null   |null|null      |null   |null   |null  |null   |null  |null  |20.5      |\n|2008|4  |20.0|22.0     |null |null|24.0 |20.5   |null|null      |null   |null   |23.0  |null   |19.5  |22.3  |21.0      |\n|1999|8  |null|13.8     |11.0 |12.8|null |null   |14.0|11.0      |11.0   |13.0   |null  |null   |null  |11.0  |null      |\n+----+---+----+---------+-----+----+-----+-------+----+----------+-------+-------+------+-------+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "mpg_pivot = mpg.groupBy(\"year\", \"cyl\").pivot(\"manufacturer\").agg(round(avg(\"cty\"), 1))\n",
    "mpg_pivot.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f7ae0f-a8ec-4770-a07b-9c5d4bccfeb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(5 marks) The file flight-summary.csv contains information about flights\n",
    "between various locations. The file airports.csv contains additional\n",
    "information about each airport, some of which is not in the flight-\n",
    "summary.csv file.\n",
    "Create a dataframe which contains the origin_code, origin_airport, latitude\n",
    "(for origin airport), longitude (for origin airport), dest_code, dest_airport,\n",
    "latitude (for destination airport), longitude (for destination airport) for every\n",
    "flight which originates in the state of Texas (TX).\n",
    "Which type of join technique ( Shuffle or Broadcast) did Spark likely use for\n",
    "this exercise? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce76cbe-7e6f-4947-be66-8cbf3bef1835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+-----------+---------+--------------------+--------+----------+\n|origin_code|      origin_airport|origin_lat|origin_long|dest_code|        dest_airport|dest_lat| dest_long|\n+-----------+--------------------+----------+-----------+---------+--------------------+--------+----------+\n|        LBB|Lubbock Preston S...|  33.66364| -101.82278|      DEN|Denver Internatio...|39.85841|  -104.667|\n|        AUS|Austin-Bergstrom ...|  30.19453|  -97.66987|      ELP|El Paso Internati...|31.80667|-106.37781|\n|        HOU|William P. Hobby ...|  29.64542|  -95.27889|      PDX|Portland Internat...|45.58872| -122.5975|\n|        DFW|Dallas/Fort Worth...|  32.89595|   -97.0372|      PNS|Pensacola Interna...|30.47331| -87.18744|\n|        DFW|Dallas/Fort Worth...|  32.89595|   -97.0372|      SDF|Louisville Intern...|38.17439|   -85.736|\n|        BPT|Jack Brooks Regio...|  29.95083|  -94.02069|      DFW|Dallas/Fort Worth...|32.89595|  -97.0372|\n|        DFW|Dallas/Fort Worth...|  32.89595|   -97.0372|      HOU|William P. Hobby ...|29.64542| -95.27889|\n|        SAT|San Antonio Inter...|  29.53369|  -98.46978|      LAS|McCarran Internat...|36.08036|-115.15233|\n|        IAH|George Bush Inter...|  29.98047|  -95.33972|      LIT|Bill and Hillary ...| 34.7294| -92.22425|\n|        IAH|George Bush Inter...|  29.98047|  -95.33972|      BZN|Bozeman Yellowsto...| 45.7769|-111.15301|\n|        DFW|Dallas/Fort Worth...|  32.89595|   -97.0372|      VPS|Destin-Fort Walto...|30.48325|  -86.5254|\n|        ELP|El Paso Internati...|  31.80667| -106.37781|      LAS|McCarran Internat...|36.08036|-115.15233|\n|        DAL|   Dallas Love Field|  32.84711|  -96.85177|      ICT|Wichita Dwight D....|37.64996| -97.43305|\n|        LBB|Lubbock Preston S...|  33.66364| -101.82278|      IAH|George Bush Inter...|29.98047| -95.33972|\n|        IAH|George Bush Inter...|  29.98047|  -95.33972|      GSP|Greenville-Sparta...|34.89567| -82.21886|\n|        DAL|   Dallas Love Field|  32.84711|  -96.85177|      IAH|George Bush Inter...|29.98047| -95.33972|\n|        IAH|George Bush Inter...|  29.98047|  -95.33972|      DEN|Denver Internatio...|39.85841|  -104.667|\n|        CRP|Corpus Christi In...|  27.77036|  -97.50122|      IAH|George Bush Inter...|29.98047| -95.33972|\n|        AMA|Rick Husband Amar...|  35.21937| -101.70593|      DAL|   Dallas Love Field|32.84711| -96.85177|\n|        AUS|Austin-Bergstrom ...|  30.19453|  -97.66987|      IAD|Washington Dulles...|38.94453| -77.45581|\n+-----------+--------------------+----------+-----------+---------+--------------------+--------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flights =  spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/flight_summary.csv\")\n",
    "\n",
    "airports =  spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/airports.csv\")\n",
    "\n",
    "flights_tx = flights.filter(col(\"origin_state\") == \"TX\")\n",
    "\n",
    "joined_df = flights_tx.join(airports.alias(\"orig\"), flights_tx[\"origin_code\"] == col(\"orig.IATA_CODE\"), \"left\") \\\n",
    "    .join(airports.alias(\"dest\"), flights_tx[\"dest_code\"] == col(\"dest.IATA_CODE\"), \"left\") \\\n",
    "    .select(flights_tx[\"origin_code\"], \n",
    "        col(\"orig.AIRPORT\").alias(\"origin_airport\"), col(\"orig.LATITUDE\").alias(\"origin_lat\"), col(\"orig.LONGITUDE\").alias(\"origin_long\"),\n",
    "        flights_tx[\"dest_code\"], \n",
    "        col(\"dest.AIRPORT\").alias(\"dest_airport\"), col(\"dest.LATITUDE\").alias(\"dest_lat\"), col(\"dest.LONGITUDE\").alias(\"dest_long\")\n",
    ")\n",
    "\n",
    "joined_df.show()\n",
    "\n",
    "# Spark is likely using a Broadcast Join since the airports dataset is small, as broadcasting helps avoid costly shuffles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0fafcd-de08-4431-9189-4a6fb4b66019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(5 marks) The file aapl-2017.csv contains information about daily apple stock\n",
    "prices. Calculate the weekly average price using a window function inside\n",
    "the groupBy transformation. Order the result by the start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b946ad0-315e-47a9-9903-0ac514d0587d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n|week|start_date|weekly_avg_price  |\n+----+----------+------------------+\n|1   |2017-01-03|116.67250100000001|\n|2   |2017-01-09|119.228           |\n|3   |2017-01-17|119.94249925      |\n|4   |2017-01-23|121.16399980000001|\n|5   |2017-01-30|125.86799920000001|\n|6   |2017-02-06|131.6799956       |\n|7   |2017-02-13|134.97799980000002|\n|8   |2017-02-21|136.75000025      |\n|9   |2017-02-27|138.4899994       |\n|10  |2017-03-06|139.1359984       |\n+----+----------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "stocks =  spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/aapl_2017.csv\")\n",
    "\n",
    "aapl_weekly = stocks.withColumn(\"week\", weekofyear(\"Date\"))\n",
    "\n",
    "weekly_window = Window.partitionBy(\"week\")\n",
    "\n",
    "aweekly_avg = aapl_weekly.withColumn(\"weekly_avg_price\", avg(\"Close\").over(weekly_window))\n",
    "\n",
    "aapl_weekly_start = aapl_weekly.groupBy(\"week\") \\\n",
    "    .agg(min(\"Date\").alias(\"start_date\"), avg(\"Close\").alias(\"weekly_avg_price\")) \\\n",
    "    .orderBy(\"start_date\")\n",
    "\n",
    "aapl_weekly_start.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CP1370 Lab 07",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
