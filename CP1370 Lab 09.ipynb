{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db09a890-116c-48c3-964f-d84b3c46d0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Create folder named “mobile” in dbfs on databricks. Place the files ‘file1.json’, ‘file2.json’, ‘file3.json’ in this folder. Copy the path to the folder.\n",
    "\n",
    "Step 2: In a notebook add in the following code: ( be sure to substitute the correct path to the mobile\n",
    "folder)\n",
    "\n",
    "Step 3: Run the code to start a streaming spark job. ( 2 marks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b0091b-7a2c-49a2-983d-ed2d43595e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "mobile_folder_path = 'dbfs:/FileStore/mobile'\n",
    "\n",
    "# Define the schema\n",
    "mobile_data_schema = StructType() \\\n",
    "  .add(\"id\", StringType(), False) \\\n",
    "  .add(\"action\", StringType(), False) \\\n",
    "  .add(\"ts\", TimestampType(), False)\n",
    "\n",
    "# Create a streaming DataFrame from JSON files in the specified directory\n",
    "mobile_ss_df = spark.readStream \\\n",
    "  .schema(mobile_data_schema) \\\n",
    "  .json(mobile_folder_path)\n",
    "\n",
    "# Check if the DataFrame is a streaming DataFrame\n",
    "print(mobile_ss_df.isStreaming) # Should print True\n",
    "\n",
    "# Group by a 10-minute window and action, then count occurrences\n",
    "action_count_df = mobile_ss_df.groupBy(window(\"ts\", \"10 minutes\"),\"action\").count()\n",
    "\n",
    "# Write the output to the console\n",
    "mobile_console_sq = action_count_df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b14d3007-3f31-4f38-9717-c711fc8ed78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. In a new code cell, display the results of the streaming job by calling\n",
    "“display(action_count_df)”. ( 1 mark )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5aa3725-524c-41eb-b732-e38c4e03a0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>window</th><th>action</th><th>count</th></tr></thead><tbody><tr><td>List(2018-03-02T10:00:00.000+0000, 2018-03-02T10:10:00.000+0000)</td><td>open</td><td>4</td></tr><tr><td>List(2018-03-02T11:00:00.000+0000, 2018-03-02T11:10:00.000+0000)</td><td>crash</td><td>1</td></tr><tr><td>List(2018-03-02T10:10:00.000+0000, 2018-03-02T10:20:00.000+0000)</td><td>open</td><td>1</td></tr><tr><td>List(2018-03-02T10:00:00.000+0000, 2018-03-02T10:10:00.000+0000)</td><td>close</td><td>3</td></tr><tr><td>List(2018-03-02T11:10:00.000+0000, 2018-03-02T11:20:00.000+0000)</td><td>swipe</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2018-03-02T10:00:00.000+0000",
          "2018-03-02T10:10:00.000+0000"
         ],
         "open",
         4
        ],
        [
         [
          "2018-03-02T11:00:00.000+0000",
          "2018-03-02T11:10:00.000+0000"
         ],
         "crash",
         1
        ],
        [
         [
          "2018-03-02T10:10:00.000+0000",
          "2018-03-02T10:20:00.000+0000"
         ],
         "open",
         1
        ],
        [
         [
          "2018-03-02T10:00:00.000+0000",
          "2018-03-02T10:10:00.000+0000"
         ],
         "close",
         3
        ],
        [
         [
          "2018-03-02T11:10:00.000+0000",
          "2018-03-02T11:20:00.000+0000"
         ],
         "swipe",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"spark.timeWindow\":true}",
         "name": "window",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "action",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(action_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5668d020-5835-44c6-a849-52aaaebb6b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Add an additional file to the ‘mobile’ folder on dbfs. The file to add is named ‘newaction.json’\n",
    "\n",
    "6. After adding the file, in a new cell, display the results of the streaming job by calling\n",
    "‘display(action_count_df)’. ( 1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad7415d-c812-4457-9118-29775daa5053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>window</th><th>action</th><th>count</th></tr></thead><tbody><tr><td>List(2018-03-02T10:00:00.000+0000, 2018-03-02T10:10:00.000+0000)</td><td>open</td><td>4</td></tr><tr><td>List(2018-03-02T11:00:00.000+0000, 2018-03-02T11:10:00.000+0000)</td><td>crash</td><td>1</td></tr><tr><td>List(2018-03-02T10:10:00.000+0000, 2018-03-02T10:20:00.000+0000)</td><td>open</td><td>1</td></tr><tr><td>List(2018-03-02T10:00:00.000+0000, 2018-03-02T10:10:00.000+0000)</td><td>close</td><td>3</td></tr><tr><td>List(2018-03-02T11:10:00.000+0000, 2018-03-02T11:20:00.000+0000)</td><td>swipe</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2018-03-02T10:00:00.000+0000",
          "2018-03-02T10:10:00.000+0000"
         ],
         "open",
         4
        ],
        [
         [
          "2018-03-02T11:00:00.000+0000",
          "2018-03-02T11:10:00.000+0000"
         ],
         "crash",
         1
        ],
        [
         [
          "2018-03-02T10:10:00.000+0000",
          "2018-03-02T10:20:00.000+0000"
         ],
         "open",
         1
        ],
        [
         [
          "2018-03-02T10:00:00.000+0000",
          "2018-03-02T10:10:00.000+0000"
         ],
         "close",
         3
        ],
        [
         [
          "2018-03-02T11:10:00.000+0000",
          "2018-03-02T11:20:00.000+0000"
         ],
         "swipe",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"spark.timeWindow\":true}",
         "name": "window",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "action",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(action_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c144d7-fd4c-4e28-87e7-5605e06339b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. How has the input changed between step 4 and step 6? ( 2 marks)\n",
    "\n",
    "The input would have changed but i had left the cell running for step 4 so it updated at the same time. However with the newaction file added the counts would be higher, and the new action in the file would add an additional row to the databricks table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0561845-f7a9-4f05-a362-a07b5c4f5a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Stop the streaming job by calling ‘mobile_console_sq.stop()’. This stops the\n",
    "streaming query job. ( 1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001b2412-b43a-46ad-839d-d5aaed743fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mobile_console_sq.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aef5b11-767e-4506-9327-39caeac49f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. For the streaming job setup in step 2, describe the following properties:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;a) Data Source ( 2 marks )\n",
    "\n",
    "The data source for the streaming job is the directory in the DBFS that the job is reading its data from. This is where the job reads the JSON files from in a streaming fashion, meaning when a new file is added spark will read the folder and process the new data in real time.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;b) Output Mode ( 2 marks )\n",
    "\n",
    "Output mode defines how the processed data is written to the ouput after every trigger. The use of compelete in this job means that the entire reulst of the aggregation (count of actions) is re calculated and displayed every time new data is processed.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;c) Trigger Type ( 2 marks )\n",
    "\n",
    "With streaming jobs typically data is continuously flowing into the system. The trigger is the event that kicks off the processing of the data. By default, Spark uses micro-batch processing, meaning it continuously checks for new data and processes it in small batches.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;d) Data Sink ( 2 marks )\n",
    "\n",
    "The data sink in the streaming hov is the destinatoin where the processed data is written to. In our case the data is being written to the console output and the Databricks display allowing to be viewed using the display(action_count_df) function.\n",
    "\n",
    "\n",
    "10. Describe the processing which is being done by the streaming job\n",
    "( summarize in a paragraph what the streaming job does) ( 4 marks)\n",
    "\n",
    "This streaming job is continuously reading JSON files from the 'mobile' folder set up in DBFS. Each JSON file contains a record with an id, action, and a timestamp (ts). The job extracts this data and groups it into 10 minute time windows, counting the number of occurences of each action type within these windows. The processed results are then displayed in real time through the Databricks console and table view. As new files are added to our 'mobile' folder, the job will process the new additions and update the counts dynamically. The streaming job ensrues any new actions are processed and reported live, giving a good example of real-time data monitoring, collection, and analysis."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CP1370 Lab 09",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
